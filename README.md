# 🐴 国宝AI活化工作台：舞马衔杯仿皮囊式银壶

> 基于 Ollama (Qwen3-14B) + RAG 技术构建的"反人机交互"短视频创作辅助系统。

---

## 📖 项目简介

本项目属于"2025联合课程实践"项目的一部分,旨在通过 AI 技术赋能国宝文物的活化传播。

系统针对"舞马衔杯仿皮囊式银壶"这一文物,构建了一个专业的 RAG(检索增强生成)工作台。它能够辅助创作者完成《国宝画重点》栏目的短视频创意生成,特别是针对 STEP 3 阶段的"对话纠偏"任务。

## ✨ 核心功能 (Core Features)

### 1. 🧠 七大专业角色矩阵 (7-Role Cognitive Matrix)

基于《国宝画重点》联合课程的教学框架,内置七大深度定制的 AI 智能体,覆盖文物活化的全链路:

- **🧐 专家学者 / 研究型助理**: 严谨考据史实,提供结构化的学术汇报与文献分析
- **🎨 交互设计师 / 情感化设计**: 构思"反人机交互"创新玩法(如重力感应、面部追踪、声纹互动),挖掘文物的情感连接点
- **🔮 符号学者**: 深度破译"胡汉融合"、"舞马祝寿"等纹样背后的深层文化隐喻与哲学内涵
- **📢 策展人 / 用户体验**: 优化抖音平台的短视频传播策略(完播率、话题性),绘制用户体验蓝图

---

### 2. 📚 动态 RAG 知识库引擎 (Dynamic RAG Engine)

抛弃静态数据的限制,构建可生长的知识大脑:

- **实时投喂**: 支持前端直接上传 PDF / Markdown / TXT 格式的学术资料、论文或策展方案
- **自动向量化**: 后台自动完成文本切分 (Chunking) 与 Embedding 向量化,即刻存入 ChromaDB
- **可视化管理**: 提供透明的知识库列表,支持随时查看与一键删除过时文档,确保 AI 知识库的纯净度与时效性

---

### 3. 🛡️ 史实纠偏与考据护航 (Fact-Checking & Grounding)

针对 AIGC 常见的"幻觉"问题,建立双重保障机制:

- **检索增强**: AI 在回答前必须先检索本地向量库中的权威资料,确保"言之有据"
- **主动纠偏**: 当用户的创意方案(如朝代、形制、工艺)与史实冲突时,系统会温和指出并提供学术依据,确保新华社级别的发布严谨性

---

### 4. 💭 白盒化思维链展示 (Visible Chain-of-Thought)

拒绝黑盒体验,让 AI 的思考过程可见:

- **深度思考**: 集成大模型的深度推理能力,能够展示 AI 如何拆解问题、分析隐喻、规划逻辑
- **交互式折叠**: 前端界面支持 `<think>` 标签的解析与折叠显示,用户既可查看 AI 的推理路径,也能直接获取最终的精炼结论

---

### 5. 🔒 全私有化安全部署 (Fully Private Deployment)

- **数据不出域**: 基于 Ollama (Qwen3-14B) + ChromaDB 的纯本地架构
- **零依赖运行**: 所有模型推理、数据存储、向量检索均在自有服务器完成,无需上传云端,严格保障课题组的创意资产与数据安全

---

## 📂 项目结构

```text
project_root/
├── knowledge_base/          # 📚 知识库目录
│   └── silver_flask.md      # 舞马衔杯银壶的核心资料(Markdown格式)
├── chroma_db/               # 🗄️ 向量数据库(运行脚本后自动生成)
├── chat_history/            # 💾 对话记录自动保存目录
├── ingest.py                # ⚙️ 数据入库脚本(只需运行一次)
├── app.py                   # 🖥️ Web 应用主程序
├── requirements.txt         # 📦 依赖列表
└── README.md                # 📄 项目说明书
```

---

## 🚀 运行与维护指南

### 1.创建与激活环境
```bash
#创建环境
python -m venv venv

#激活环境
source venv/bin/activate
```

### 2. 启动应用 (后台挂载)

为了确保关闭 SSH 终端后应用继续运行,并允许局域网访问,请使用 `nohup` 命令启动:

```bash
nohup streamlit run app.py --server.port 8501 --server.address 0.0.0.0 > streamlit.log 2>&1 &
```

**参数说明:**
- `--server.port 8501`: 指定运行端口
- `--server.address 0.0.0.0`: **关键参数**,允许外部 IP(如局域网内的同学)访问,而不仅仅是本机
- `> streamlit.log 2>&1`: 将所有输出日志保存在 `streamlit.log` 文件中,方便排查问题
- `&`: 让程序在后台运行

---

### 2. 查看运行状态和日志

要查看 Streamlit 是否正在运行,或者查找它的进程号 (PID):

```bash
#查看是否运行
ps aux | grep streamlit
```

你将看到类似如下的输出,第二列的数字即为 PID:

```
wuzz 12345 1.0 2.5 ... python streamlit run app.py ...
```
```bash
#实时查看日志
tail -f streamlit.log
```

---

### 3. 停止/重启应用

如果需要停止服务或重启应用,请先找到 PID,然后使用 `kill` 命令:

```bash
# 语法: kill -9 <PID>
kill -9 12345  # 将 12345 替换为你实际查到的进程号
```

---

### 4. 如何访问网页

#### 🏫 场景 A: 在学校/实验室 (局域网直连)

如果你和服务器连接的是同一个学校网络(WiFi 或有线),可以直接通过服务器内网 IP 访问:

👉 `http://<你的服务器IP>:8501`

**示例:** `http://10.122.202.53:8501`

---

#### 🌍 场景 B: 在校外 (外网访问)

如果在校外无法直接连接内网 IP,请配合 Cloudflare Tunnel 使用:

1. 确保 Streamlit 已按上述方式启动
2. 运行穿透工具:

```bash
./cloudflared-linux-amd64 tunnel --url http://localhost:8501
```

3. 使用生成的 `.trycloudflare.com` 链接访问

---

## 🛠️ 快速部署流程

### 步骤 1: 环境安装

```bash
pip install -r requirements.txt
```

### 步骤 2: 模型准备 (Ollama)

```bash
ollama pull qwen3:14b
ollama pull nomic-embed-text
```

### 步骤 3: 构建知识库

```bash
python3 ingest.py
```

### 步骤 4: 启动服务

参考上方 **"启动应用"** 章节执行启动命令

---

## 📝 注意事项

- 首次运行 `ingest.py` 时会创建向量数据库,可能需要几分钟时间
- 对话记录会自动保存在 `chat_history/` 目录下
- 如遇到端口占用,可修改启动命令中的端口号(如改为 8502)
- 日志文件 `streamlit.log` 可用于调试和监控应用状态

---

## 🤝 技术支持

如有问题,请检查:
1. Ollama 服务是否正常运行: `ollama list`
2. 所需模型是否已下载
3. 防火墙是否开放 8501 端口(局域网访问时)