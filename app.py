import os

# --- 0. åŸºç¡€é…ç½®ä¸ç¯å¢ƒè®¾ç½® ---
# å¿…é¡»åœ¨å¯¼å…¥ä»»ä½• langchain/chromadb åº“ä¹‹å‰è®¾ç½®
os.environ["ANONYMIZED_TELEMETRY"] = "False"
os.environ["ALLOW_RESET"] = "True"

import streamlit as st
import json
import glob
import re
import shutil
from datetime import datetime

# LangChain & RAG åº“
from langchain_community.chat_models import ChatOllama
from langchain_community.embeddings import OllamaEmbeddings
from langchain_community.vectorstores import Chroma
from langchain.prompts import ChatPromptTemplate
from langchain.schema.runnable import RunnablePassthrough
from langchain.schema.output_parser import StrOutputParser
from langchain.text_splitter import RecursiveCharacterTextSplitter
from langchain_community.document_loaders import PyPDFLoader, TextLoader, UnstructuredMarkdownLoader

# --- é¡µé¢é…ç½® ---
st.set_page_config(
    page_title="å›½å®AIæ´»åŒ–å·¥ä½œå° (Pro Maxç‰ˆ)",
    page_icon="ğŸº",
    layout="wide"
)

# å®šä¹‰è·¯å¾„
CHROMA_DB_DIR = "./chroma_db"
HISTORY_DIR = "./chat_history"
KNOWLEDGE_BASE_DIR = "./knowledge_base"  # æ–°å¢ï¼šçŸ¥è¯†åº“æºæ–‡ä»¶å­˜å‚¨ç›®å½•
OLLAMA_URL = "http://localhost:11434"

# ç¡®ä¿ç›®å½•å­˜åœ¨
for directory in [HISTORY_DIR, CHROMA_DB_DIR, KNOWLEDGE_BASE_DIR]:
    if not os.path.exists(directory):
        os.makedirs(directory)

# --- 1. æ·±åº¦è§’è‰²å®šä¹‰ ---
ROLE_DEFINITIONS = {
    "ä¸“å®¶å­¦è€…": {
        "description": "åƒä¸€ä½å­¦æœ¯æ³°æ–—ã€‚ä½ çš„å›ç­”åº”å½“ç³»ç»ŸåŒ–ã€é€»è¾‘ä¸¥å¯†ã€è®ºè¿°å……åˆ†ã€‚",
        "focus": "å†å²ä¸è€ƒå¤ã€å·¥è‰ºä¸ææ–™ã€æ–‡çŒ®è€ƒæ®ã€‚",
        "instruction": "ç›´æ¥å†™'ä»‹ç»ä¸€ä¸‹...'æ—¶ï¼Œä¸è¦æ³›æ³›è€Œè°ˆï¼Œè¦åƒåšä¸“é¢˜æŠ¥å‘Šä¸€æ ·ã€‚å¼•ç”¨è€ƒå¤å‘æ˜æˆæœï¼Œåˆ†æå…¶åœ¨å†å²é•¿æ²³ä¸­çš„åœ°ä½ã€‚"
    },
    "ç ”ç©¶å‹åŠ©ç†": {
        "description": "é«˜æ•ˆçš„ç ”ç©¶åŠ©æ‰‹ã€‚ä½ çš„ä»»åŠ¡æ˜¯å¿«é€Ÿã€å‡†ç¡®åœ°æœé›†ã€æ•´ç†ã€åŠ å·¥ä¿¡æ¯ã€‚",
        "focus": "èµ„æ–™æ±‡æ€»ã€æ•°æ®åˆ†æã€æ¡ç†æ¸…æ™°ã€‚",
        "instruction": "ä½ å¯ä»¥è‡ªä¸»é€‰æ‹©åˆ†æè§’åº¦ã€‚å½¢æˆçš„æ±‡æŠ¥ææ–™å¿…é¡»ç»“æ„æ¸…æ™°ï¼ˆä½¿ç”¨Markdownåˆ—è¡¨ï¼‰ï¼Œå¸®åŠ©ç”¨æˆ·å¿«é€Ÿç†è§£æ ¸å¿ƒçŸ¥è¯†ç‚¹ã€‚"
    },
    "äº¤äº’è®¾è®¡å¸ˆ": {
        "description": "å‰ç»æ€§çš„ä½“éªŒåˆ›é€ è€…ã€‚å…³æ³¨'å¦‚ä½•è®©æ–‡ç‰©è¢«æ„ŸçŸ¥'å’Œ'è§‚ä¼—å°†å¦‚ä½•ä½“éªŒå®ƒ'ã€‚",
        "focus": "å±•è§ˆäº¤äº’è®¾è®¡ã€æ²‰æµ¸ä½“éªŒã€æ•°å­—ç©ºé—´(AR/VR)ã€äº”æ„Ÿä½“éªŒã€‚",
        "instruction": "åŸºäºåˆ›æ„å’ŒæŠ€æœ¯ï¼Œæå‡ºä½“éªŒæ€§çš„è§£å†³æ–¹æ¡ˆã€‚æ€è€ƒå¦‚ä½•é€šè¿‡æ‰‹åŠ¿ã€å£°éŸ³ã€è§¦æ§ç­‰åäººæœºäº¤äº’æ‰‹æ®µï¼Œè®©æ–‡ç‰©'æ´»'èµ·æ¥ã€‚"
    },
    "ç¬¦å·å­¦è€…": {
        "description": "è·¨æ—¶ç©ºçš„åˆ›æ„é¡¾é—®ã€‚æ·±åº¦ç ´è¯‘æ–‡ç‰©ç¬¦å·çš„æ–‡åŒ–ä¸å“²å­¦å†…æ¶µã€‚",
        "focus": "ç¬¦å·ä¸éšå–»ã€çº¹æ ·è§£è¯»ã€å®—æ•™ä¸ä»ªå¼ã€å“²å­¦ä¸è®¾è®¡ã€‚",
        "instruction": "ä¸è¦åªçœ‹è¡¨é¢ï¼Œè¦è§£è¯»ç¬¦å·èƒŒåçš„éšå–»ï¼ˆå¦‚é¾™ä»£è¡¨çš‡æƒï¼‰ã€‚å°†å¤è€çš„æ™ºæ…§è½¬åŒ–ä¸ºç°ä»£è®¾è®¡çš„æ–°é¢–å™äº‹ã€‚"
    },
    "ç”¨æˆ·ä½“éªŒç ”ç©¶å‘˜": {
        "description": "å…³æ³¨ç”¨æˆ·ç—›ç‚¹ä¸éœ€æ±‚çš„ç ”ç©¶è€…ã€‚ç³»ç»Ÿæ€§åˆ†æç”¨æˆ·åœ¨äº’åŠ¨å‰ã€ä¸­ã€åçš„æ‰€æœ‰è§¦ç‚¹ã€‚",
        "focus": "ç”¨æˆ·ç”»åƒã€ç—›ç‚¹åˆ†æã€è§¦ç‚¹(Touchpoints)ã€æœåŠ¡è“å›¾ã€‚",
        "instruction": "æä¾›ç»“æ„åŒ–çš„ç ”ç©¶å»ºè®®ã€‚æ€è€ƒç”¨æˆ·ä¸ºä»€ä¹ˆ'çœ‹ä¸æ‡‚'ã€'è®°ä¸ä½'ã€‚å…³æ³¨å¦‚ä½•é€šè¿‡è®¾è®¡è§£å†³è¿™äº›è®¤çŸ¥éšœç¢ã€‚"
    },
    "ç­–å±•äºº": {
        "description": "æ•…äº‹çš„è®²è¿°è€…ä¸ä¼ æ’­è€…ã€‚æ€è€ƒå¦‚ä½•è®©æ–‡ç‰©è¿›å…¥å¤§ä¼—è§†é‡å¹¶æˆä¸º'çˆ†æ¬¾'ã€‚",
        "focus": "å™äº‹ä¼ æ’­ã€å“ç‰Œæ–‡åŒ–ã€ç¤¾ä¼šçƒ­ç‚¹ç»“åˆã€çŸ­è§†é¢‘ä¼ æ’­ã€‚",
        "instruction": "æ€è€ƒå¦‚ä½•å°†å­¦æœ¯å†…å®¹è½¬åŒ–ä¸ºå¤§ä¼—å¯æ„ŸçŸ¥çš„'æ•…äº‹'ã€‚å…³æ³¨æŠ–éŸ³ç­‰å¹³å°çš„ä¼ æ’­è§„å¾‹ï¼ˆå®Œæ’­ç‡ã€è¯é¢˜æ€§ï¼‰ã€‚"
    },
    "æƒ…æ„ŸåŒ–è®¾è®¡ç ”ç©¶å‘˜": {
        "description": "å¿ƒç†å­¦ä¸è®¾è®¡çš„ç»“åˆè€…ã€‚å…³æ³¨'åŠŸèƒ½'å¦‚ä½•è½¬åŒ–ä¸º'æƒ…æ„Ÿè¿æ¥'ã€‚",
        "focus": "æƒ…æ„Ÿå…±é¸£ã€å¿ƒç†å­¦ã€è¯—æ„è¡¨è¾¾ã€ç”Ÿå‘½ä½“éªŒã€‚",
        "instruction": "æŒ–æ˜æ–‡ç‰©èƒŒåçš„æƒ…æ„Ÿä»·å€¼ã€‚æ€è€ƒå¦‚ä½•é€šè¿‡è®¾è®¡å¼•å‘è§‚ä¼—çš„'æƒŠå¹'ã€'æ„ŸåŠ¨'æˆ–'æ²‰æ€'ã€‚"
    }
}

PROFESSIONAL_ANGLES = """
åœ¨åˆ†ææ—¶ï¼Œè¯·ç»¼åˆè€ƒè™‘ä»¥ä¸‹ç»´åº¦ï¼š
1. [å†å²ä¸è€ƒå¤]: å‡ºåœŸèƒŒæ™¯ã€æ–­ä»£ä¾æ®ã€ç¤¾ä¼šåˆ¶åº¦ã€‚
2. [å·¥è‰ºä¸ææ–™]: åˆ¶ä½œæ–¹æ³•ã€ææ–™æ¥æºã€æŠ€æœ¯æ°´å¹³ã€‚
3. [è‰²å½©]: é¢œè‰²ç¾å­¦ã€ç­‰çº§è±¡å¾ã€çŸ¿ç‰©é¢œæ–™ã€‚
4. [å™¨å½¢]: åŠŸèƒ½ä¸å®¡ç¾çš„ç»“åˆã€ç¤¼åˆ¶è§„èŒƒã€‚
5. [ç¬¦å·ä¸éšå–»]: çº¹æ ·çš„æ–‡åŒ–å¯“æ„ã€å®—æ•™å†…æ¶µã€‚
6. [è¯—æ„è¡¨è¾¾]: ç›¸å…³çš„è¯—è¯æ­Œèµ‹ã€æ–‡å­¦æ„è±¡ã€‚
7. [æ•°å­—ç©ºé—´]: AR/VRã€åäººæœºäº¤äº’çš„å¯èƒ½æ€§ã€‚
"""


# --- 2. æ ¸å¿ƒåŠŸèƒ½å‡½æ•° ---

def parse_and_render_message(text):
    """è§£æ <think> æ ‡ç­¾å¹¶æ¸²æŸ“"""
    pattern = r"<think>(.*?)</think>(.*)"
    match = re.search(pattern, text, re.DOTALL)
    if match:
        thought, answer = match.group(1).strip(), match.group(2).strip()
        if thought:
            with st.expander("ğŸ’­ æ¨¡å‹æ€è€ƒè¿‡ç¨‹ (ç‚¹å‡»å±•å¼€)", expanded=False):
                st.markdown(thought)
        if answer:
            st.markdown(answer)
        else:
            st.info("æ¨¡å‹ä»…è¾“å‡ºäº†æ€è€ƒè¿‡ç¨‹ã€‚")
    else:
        clean_text = text.replace("<think>", "**[å¼€å§‹æ€è€ƒ]**\n").replace("</think>", "\n**[æ€è€ƒç»“æŸ]**\n")
        st.markdown(clean_text)


def get_vector_store():
    """è·å–å‘é‡æ•°æ®åº“å®ä¾‹"""
    embeddings = OllamaEmbeddings(base_url=OLLAMA_URL, model="nomic-embed-text")
    return Chroma(persist_directory=CHROMA_DB_DIR, embedding_function=embeddings)


def process_uploaded_file(uploaded_file):
    """å¤„ç†ä¸Šä¼ æ–‡ä»¶ï¼šä¿å­˜åˆ°ç›®å½• -> åŠ è½½ -> åˆ‡åˆ† -> å­˜å…¥æ•°æ®åº“"""
    try:
        # 1. ä¿å­˜æ–‡ä»¶åˆ° knowledge_base ç›®å½• (æŒä¹…åŒ–å­˜å‚¨)
        file_path = os.path.join(KNOWLEDGE_BASE_DIR, uploaded_file.name)
        with open(file_path, "wb") as f:
            f.write(uploaded_file.getbuffer())

        # 2. é€‰æ‹©åŠ è½½å™¨
        suffix = os.path.splitext(uploaded_file.name)[1].lower()
        if suffix == ".pdf":
            loader = PyPDFLoader(file_path)
        elif suffix == ".md":
            loader = UnstructuredMarkdownLoader(file_path)
        else:
            loader = TextLoader(file_path, autodetect_encoding=True)

        docs = loader.load()

        # 3. åˆ‡åˆ†æ–‡æ¡£
        text_splitter = RecursiveCharacterTextSplitter(chunk_size=800, chunk_overlap=100)
        chunks = text_splitter.split_documents(docs)

        # 4. å­˜å…¥æ•°æ®åº“
        vector_store = get_vector_store()
        vector_store.add_documents(chunks)
        vector_store.persist()

        return True, f"âœ… æˆåŠŸå…¥åº“ï¼š{len(chunks)} ä¸ªçŸ¥è¯†å—"
    except Exception as e:
        return False, str(e)


def delete_document(filename):
    """åˆ é™¤æ–‡æ¡£ï¼šä»æ•°æ®åº“ç§»é™¤å‘é‡ -> ä»ç£ç›˜åˆ é™¤æ–‡ä»¶"""
    try:
        file_path = os.path.join(KNOWLEDGE_BASE_DIR, filename)

        # 1. ä» ChromaDB ä¸­åˆ é™¤ (æ ¹æ® source metadata)
        vector_store = get_vector_store()
        # Chroma çš„ collection.delete å¯ä»¥æ ¹æ® where æ¡ä»¶åˆ é™¤
        vector_store._collection.delete(where={"source": file_path})
        vector_store.persist()

        # 2. ä»ç£ç›˜åˆ é™¤æ–‡ä»¶
        if os.path.exists(file_path):
            os.remove(file_path)
            return True, f"ğŸ—‘ï¸ å·²åˆ é™¤: {filename}"
        else:
            return True, f"âš ï¸ æ–‡ä»¶å·²ä»åº“ä¸­ç§»é™¤ï¼Œä½†ç£ç›˜ä¸Šæœªæ‰¾åˆ°åŸæ–‡ä»¶: {filename}"

    except Exception as e:
        return False, f"åˆ é™¤å¤±è´¥: {str(e)}"


def get_uploaded_files():
    """è·å–å·²ä¸Šä¼ çš„æ–‡ä»¶åˆ—è¡¨"""
    if not os.path.exists(KNOWLEDGE_BASE_DIR):
        return []
    return sorted(os.listdir(KNOWLEDGE_BASE_DIR))


# å†å²è®°å½•ç®¡ç†å‡½æ•° (ä¿æŒä¸å˜)
def save_chat_history(chat_id, messages):
    with open(os.path.join(HISTORY_DIR, f"{chat_id}.json"), "w", encoding="utf-8") as f:
        json.dump(messages, f, ensure_ascii=False, indent=4)


def load_chat_history(chat_id):
    path = os.path.join(HISTORY_DIR, f"{chat_id}.json")
    return json.load(open(path, "r", encoding="utf-8")) if os.path.exists(path) else []


def get_chat_history_list():
    files = glob.glob(os.path.join(HISTORY_DIR, "*.json"))
    files.sort(key=os.path.getmtime, reverse=True)
    return files


def get_chat_title(messages):
    for msg in messages:
        if msg["role"] == "user":
            return msg["content"][:12] + "..." if len(msg["content"]) > 12 else msg["content"]
    return "æ–°å¯¹è¯"


def init_new_chat():
    st.session_state.current_chat_id = datetime.now().strftime("%Y%m%d_%H%M%S")
    st.session_state.messages = [{"role": "assistant", "content": "ä½ å¥½ï¼æˆ‘æ˜¯å›½å®æ´»åŒ–åŠ©æ‰‹ã€‚è¯·ä¸Šä¼ èµ„æ–™æˆ–ç›´æ¥æé—®ã€‚"}]


# --- 3. åˆå§‹åŒ– Session ---
if "current_chat_id" not in st.session_state:
    init_new_chat()


# --- 4. åŠ è½½èµ„æº ---
@st.cache_resource
def load_resources():
    embeddings = OllamaEmbeddings(base_url=OLLAMA_URL, model="nomic-embed-text")
    llm = ChatOllama(base_url=OLLAMA_URL, model="qwen3:14b", temperature=0.5)
    return embeddings, llm


try:
    embeddings, llm = load_resources()
except Exception as e:
    st.error(f"æ¨¡å‹è¿æ¥å¤±è´¥: {e}")
    st.stop()

# --- 5. ä¾§è¾¹æ  UI ---
with st.sidebar:
    st.title("ğŸº å›½å®AIæ´»åŒ–")

    # === æ¨¡å— A: çŸ¥è¯†åº“ç®¡ç† ===
    with st.expander("ğŸ“š çŸ¥è¯†åº“ç®¡ç† (ä¸Šä¼ /æŸ¥çœ‹/åˆ é™¤)", expanded=False):
        # 1. ä¸Šä¼ åŒºåŸŸ
        uploaded_file = st.file_uploader("ä¸Šä¼ æ–°èµ„æ–™ (PDF/MD/TXT)", type=["pdf", "md", "txt"])
        if uploaded_file and st.button("å¼€å§‹å­¦ä¹ ", key="upload_btn"):
            with st.spinner("æ­£åœ¨é˜…è¯»å¹¶å­˜å…¥å¤§è„‘..."):
                success, msg = process_uploaded_file(uploaded_file)
                if success:
                    st.success(msg)
                    st.cache_resource.clear()  # æ¸…é™¤ç¼“å­˜ï¼Œç¡®ä¿ä¸‹æ¬¡æ£€ç´¢èƒ½ç”¨æ–°æ•°æ®
                    st.rerun()  # åˆ·æ–°é¡µé¢æ˜¾ç¤ºæ–°æ–‡ä»¶åˆ—è¡¨
                else:
                    st.error(f"å­¦ä¹ å¤±è´¥: {msg}")

        st.divider()

        # 2. æ–‡ä»¶åˆ—è¡¨ä¸åˆ é™¤åŒºåŸŸ
        st.caption("ğŸ“‚ å·²æ”¶å½•æ–‡æ¡£åˆ—è¡¨")
        existing_files = get_uploaded_files()

        if not existing_files:
            st.info("æš‚æ— æ–‡æ¡£")
        else:
            for filename in existing_files:
                col_f1, col_f2 = st.columns([0.8, 0.2])
                with col_f1:
                    st.text(filename)
                with col_f2:
                    if st.button("âŒ", key=f"del_doc_{filename}", help="åˆ é™¤æ­¤æ–‡æ¡£"):
                        with st.spinner("æ­£åœ¨åˆ é™¤..."):
                            success, msg = delete_document(filename)
                            if success:
                                st.success(msg)
                                st.cache_resource.clear()
                                st.rerun()
                            else:
                                st.error(msg)

    st.divider()

    # === æ¨¡å— B: å¯¹è¯ç®¡ç† ===
    if st.button("â• æ–°å»ºå¯¹è¯", use_container_width=True):
        init_new_chat()
        st.rerun()

    st.subheader("ğŸ“œ å†å²è®°å½•")
    files = get_chat_history_list()
    if not files: st.caption("æš‚æ— è®°å½•")

    for file_path in files:
        file_name = os.path.basename(file_path)
        chat_id = file_name.replace(".json", "")
        try:
            with open(file_path, "r", encoding="utf-8") as f:
                msgs = json.load(f)
            title = get_chat_title(msgs)
            date_str = f"{chat_id[4:6]}/{chat_id[6:8]} {chat_id[9:11]}:{chat_id[11:13]}"
        except:
            title = "æœªçŸ¥å¯¹è¯"
            date_str = ""

        col1, col2 = st.columns([0.85, 0.15])
        with col1:
            prefix = "ğŸ“‚ " if st.session_state.current_chat_id == chat_id else ""
            if st.button(f"{prefix}{title}\n{date_str}", key=f"load_{chat_id}", use_container_width=True):
                st.session_state.current_chat_id = chat_id
                st.session_state.messages = msgs
                st.rerun()
        with col2:
            if st.button("ğŸ—‘", key=f"del_{chat_id}"):
                os.remove(file_path)
                if st.session_state.current_chat_id == chat_id: init_new_chat()
                st.rerun()

    st.divider()

    # === æ¨¡å— C: è§’è‰²é€‰æ‹© ===
    selected_role = st.selectbox("ğŸ­ é€‰æ‹©åˆ†æè§’è‰²", list(ROLE_DEFINITIONS.keys()))
    role_info = ROLE_DEFINITIONS[selected_role]
    st.info(f"**{selected_role}**\n\n{role_info['description']}")

# --- 6. ä¸»ç•Œé¢ä¸ RAG é€»è¾‘ ---

st.header(f"å½“å‰ä¼šè¯: {get_chat_title(st.session_state.messages)}")

# 6.1 åŠ¨æ€åŠ è½½æ•°æ®åº“
if os.path.exists(CHROMA_DB_DIR):
    # æ¯æ¬¡é‡æ–°åŠ è½½ vector_store ä»¥ç¡®ä¿è·å–æœ€æ–°çŠ¶æ€
    vector_store = get_vector_store()
    try:
        cnt = vector_store._collection.count()
        # å¦‚æœæ–‡æ¡£å¾ˆå°‘ï¼Œå°±å‡å°‘ k å€¼ï¼Œé¿å…æŠ¥é”™
        k = min(4, cnt) if cnt > 0 else 0
    except:
        k = 0

    if k > 0:
        retriever = vector_store.as_retriever(search_kwargs={"k": k})
    else:
        retriever = None
else:
    retriever = None

# 6.2 æ¸²æŸ“å†å²æ¶ˆæ¯
for msg in st.session_state.messages:
    with st.chat_message(msg["role"]):
        if msg["role"] == "assistant":
            parse_and_render_message(msg["content"])
        else:
            st.markdown(msg["content"])

# 6.3 æ„å»º Prompt
role_config = ROLE_DEFINITIONS[selected_role]

system_template = f"""
ä½ ç°åœ¨çš„èº«ä»½æ˜¯ï¼š**{selected_role}**ã€‚
{role_config['description']}

**ä½ çš„æ ¸å¿ƒå…³æ³¨ç‚¹ï¼š**
{role_config['focus']}

**å›å¤æŒ‡å¯¼åŸåˆ™ï¼š**
{role_config['instruction']}

**ä¸“ä¸šåˆ†æç»´åº¦å‚è€ƒï¼š**
{PROFESSIONAL_ANGLES}

è¯·æ ¹æ®ä»¥ä¸‹ã€å‚è€ƒèµ„æ–™ã€‘æ¥å›ç­”ç”¨æˆ·çš„ã€é—®é¢˜ã€‘ã€‚
å¦‚æœèµ„æ–™ä¸­æ²¡æœ‰ç­”æ¡ˆï¼Œè¯·è¿ç”¨ä½ çš„ä¸“ä¸šçŸ¥è¯†è¿›è¡Œåˆç†æ¨æ¼”ï¼Œä½†å¿…é¡»å£°æ˜è¿™æ˜¯æ¨æ¼”ã€‚

---
ã€å‚è€ƒèµ„æ–™ã€‘ï¼š
{{context}}
---
ã€ç”¨æˆ·é—®é¢˜ã€‘ï¼š
{{question}}
"""

prompt = ChatPromptTemplate.from_template(system_template)

# 6.4 å¤„ç†è¾“å…¥
if user_input := st.chat_input("è¯·è¾“å…¥å…³äºæ–‡ç‰©çš„åˆ›æ„æˆ–é—®é¢˜..."):
    with st.chat_message("user"):
        st.markdown(user_input)
    st.session_state.messages.append({"role": "user", "content": user_input})
    save_chat_history(st.session_state.current_chat_id, st.session_state.messages)

    with st.chat_message("assistant"):
        placeholder = st.empty()
        full_response = ""

        if retriever:
            chain = (
                    {"context": retriever | (lambda docs: "\n\n".join(d.page_content for d in docs)),
                     "question": RunnablePassthrough()}
                    | prompt
                    | llm
                    | StrOutputParser()
            )
        else:
            chain = (
                    {"context": lambda x: "æš‚æ— æœ¬åœ°çŸ¥è¯†åº“ï¼Œè¯·ä¾é é€šç”¨çŸ¥è¯†å›ç­”ã€‚", "question": RunnablePassthrough()}
                    | prompt
                    | llm
                    | StrOutputParser()
            )

        try:
            with st.spinner(f"{selected_role} æ­£åœ¨è°ƒåŠ¨çŸ¥è¯†åº“è¿›è¡Œåˆ†æ..."):
                for chunk in chain.stream(user_input):
                    full_response += chunk
                    placeholder.markdown(full_response + "â–Œ")

                placeholder.empty()
                parse_and_render_message(full_response)
        except Exception as e:
            st.error(f"ç”Ÿæˆå‡ºé”™: {e}")
            full_response = "æŠ±æ­‰ï¼Œç³»ç»Ÿå‡ºäº†ç‚¹å°å·®é”™ã€‚"

    st.session_state.messages.append({"role": "assistant", "content": full_response})
    save_chat_history(st.session_state.current_chat_id, st.session_state.messages)

# CSS ä¼˜åŒ–
st.markdown("""
<style>
    .stButton>button {border-radius: 8px;}
    div[data-testid="stExpander"] div[data-testid="stVerticalBlock"] button {
        text-align: left; border: 1px solid #eee;
    }
    .streamlit-expanderHeader {
        background-color: #f8f9fa; border-radius: 5px; font-size: 0.9em;
    }
    /* è°ƒæ•´åˆ é™¤æŒ‰é’®æ ·å¼ */
    div[data-testid="column"]:nth-of-type(2) button {
        color: #ff4b4b;
        border-color: #ff4b4b;
    }
</style>
""", unsafe_allow_html=True)