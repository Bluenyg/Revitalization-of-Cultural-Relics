import os

# --- 0. åŸºç¡€é…ç½®ä¸ç¯å¢ƒè®¾ç½® ---
os.environ["ANONYMIZED_TELEMETRY"] = "False"
os.environ["ALLOW_RESET"] = "True"

import streamlit as st
import json
import glob
import re
import time
import uuid
from datetime import datetime

# LangChain & RAG åº“
from langchain_community.chat_models import ChatOllama
from langchain_community.embeddings import OllamaEmbeddings
from langchain_community.vectorstores import Chroma
from langchain.prompts import ChatPromptTemplate, PromptTemplate  # [ä¿®æ”¹] å¼•å…¥ PromptTemplate
from langchain.schema.runnable import RunnablePassthrough
from langchain.schema.output_parser import StrOutputParser
from langchain.text_splitter import RecursiveCharacterTextSplitter
from langchain_community.document_loaders import (
    PyPDFLoader,
    PDFPlumberLoader,
    TextLoader,
    UnstructuredMarkdownLoader
)

# --- é¡µé¢é…ç½® ---
st.set_page_config(
    page_title="å›½å®AIæ´»åŒ–å·¥ä½œå° (Pro Maxç‰ˆ)",
    page_icon="ğŸº",
    layout="wide"
)

# å®šä¹‰è·¯å¾„
CHROMA_DB_DIR = "./chroma_db"
HISTORY_DIR = "./chat_history"
KNOWLEDGE_BASE_DIR = "./knowledge_base"
STATUS_FILE = "./db_status.json"
OLLAMA_URL = "http://localhost:11434"

# ç¡®ä¿ç›®å½•å­˜åœ¨
for directory in [HISTORY_DIR, CHROMA_DB_DIR, KNOWLEDGE_BASE_DIR]:
    if not os.path.exists(directory):
        os.makedirs(directory)

# --- 1. æ·±åº¦è§’è‰²å®šä¹‰ (ä¿æŒä¸å˜) ---
ROLE_DEFINITIONS = {
    "ä¸“å®¶å­¦è€…": {
        "description": "åƒä¸€ä½å­¦æœ¯æ³°æ–—ã€‚ä½ çš„å›ç­”åº”å½“ç³»ç»ŸåŒ–ã€é€»è¾‘ä¸¥å¯†ã€è®ºè¿°å……åˆ†ã€‚",
        "focus": "å†å²ä¸è€ƒå¤ã€å·¥è‰ºä¸ææ–™ã€æ–‡çŒ®è€ƒæ®ã€‚",
        "instruction": "ç›´æ¥å†™'ä»‹ç»ä¸€ä¸‹...'æ—¶ï¼Œä¸è¦æ³›æ³›è€Œè°ˆï¼Œè¦åƒåšä¸“é¢˜æŠ¥å‘Šä¸€æ ·ã€‚å¼•ç”¨è€ƒå¤å‘æ˜æˆæœï¼Œåˆ†æå…¶åœ¨å†å²é•¿æ²³ä¸­çš„åœ°ä½ã€‚"
    },
    "ç ”ç©¶å‹åŠ©ç†": {
        "description": "é«˜æ•ˆçš„ç ”ç©¶åŠ©æ‰‹ã€‚ä½ çš„ä»»åŠ¡æ˜¯å¿«é€Ÿã€å‡†ç¡®åœ°æœé›†ã€æ•´ç†ã€åŠ å·¥ä¿¡æ¯ã€‚",
        "focus": "èµ„æ–™æ±‡æ€»ã€æ•°æ®åˆ†æã€æ¡ç†æ¸…æ™°ã€‚",
        "instruction": "ä½ å¯ä»¥è‡ªä¸»é€‰æ‹©åˆ†æè§’åº¦ã€‚å½¢æˆçš„æ±‡æŠ¥ææ–™å¿…é¡»ç»“æ„æ¸…æ™°ï¼ˆä½¿ç”¨Markdownåˆ—è¡¨ï¼‰ï¼Œå¸®åŠ©ç”¨æˆ·å¿«é€Ÿç†è§£æ ¸å¿ƒçŸ¥è¯†ç‚¹ã€‚"
    },
    "äº¤äº’è®¾è®¡å¸ˆ": {
        "description": "å‰ç»æ€§çš„ä½“éªŒåˆ›é€ è€…ã€‚å…³æ³¨'å¦‚ä½•è®©æ–‡ç‰©è¢«æ„ŸçŸ¥'å’Œ'è§‚ä¼—å°†å¦‚ä½•ä½“éªŒå®ƒ'ã€‚",
        "focus": "å±•è§ˆäº¤äº’è®¾è®¡ã€æ²‰æµ¸ä½“éªŒã€æ•°å­—ç©ºé—´(AR/VR)ã€äº”æ„Ÿä½“éªŒã€‚",
        "instruction": "åŸºäºåˆ›æ„å’ŒæŠ€æœ¯ï¼Œæå‡ºä½“éªŒæ€§çš„è§£å†³æ–¹æ¡ˆã€‚æ€è€ƒå¦‚ä½•é€šè¿‡æ‰‹åŠ¿ã€å£°éŸ³ã€è§¦æ§ç­‰åäººæœºäº¤äº’æ‰‹æ®µï¼Œè®©æ–‡ç‰©'æ´»'èµ·æ¥ã€‚"
    },
    "ç¬¦å·å­¦è€…": {
        "description": "è·¨æ—¶ç©ºçš„åˆ›æ„é¡¾é—®ã€‚æ·±åº¦ç ´è¯‘æ–‡ç‰©ç¬¦å·çš„æ–‡åŒ–ä¸å“²å­¦å†…æ¶µã€‚",
        "focus": "ç¬¦å·ä¸éšå–»ã€çº¹æ ·è§£è¯»ã€å®—æ•™ä¸ä»ªå¼ã€å“²å­¦ä¸è®¾è®¡ã€‚",
        "instruction": "ä¸è¦åªçœ‹è¡¨é¢ï¼Œè¦è§£è¯»ç¬¦å·èƒŒåçš„éšå–»ï¼ˆå¦‚é¾™ä»£è¡¨çš‡æƒï¼‰ã€‚å°†å¤è€çš„æ™ºæ…§è½¬åŒ–ä¸ºç°ä»£è®¾è®¡çš„æ–°é¢–å™äº‹ã€‚"
    },
    "ç”¨æˆ·ä½“éªŒç ”ç©¶å‘˜": {
        "description": "å…³æ³¨ç”¨æˆ·ç—›ç‚¹ä¸éœ€æ±‚çš„ç ”ç©¶è€…ã€‚ç³»ç»Ÿæ€§åˆ†æç”¨æˆ·åœ¨äº’åŠ¨å‰ã€ä¸­ã€åçš„æ‰€æœ‰è§¦ç‚¹ã€‚",
        "focus": "ç”¨æˆ·ç”»åƒã€ç—›ç‚¹åˆ†æã€è§¦ç‚¹(Touchpoints)ã€æœåŠ¡è“å›¾ã€‚",
        "instruction": "æä¾›ç»“æ„åŒ–çš„ç ”ç©¶å»ºè®®ã€‚æ€è€ƒç”¨æˆ·ä¸ºä»€ä¹ˆ'çœ‹ä¸æ‡‚'ã€'è®°ä¸ä½'ã€‚å…³æ³¨å¦‚ä½•é€šè¿‡è®¾è®¡è§£å†³è¿™äº›è®¤çŸ¥éšœç¢ã€‚"
    },
    "ç­–å±•äºº": {
        "description": "æ•…äº‹çš„è®²è¿°è€…ä¸ä¼ æ’­è€…ã€‚æ€è€ƒå¦‚ä½•è®©æ–‡ç‰©è¿›å…¥å¤§ä¼—è§†é‡å¹¶æˆä¸º'çˆ†æ¬¾'ã€‚",
        "focus": "å™äº‹ä¼ æ’­ã€å“ç‰Œæ–‡åŒ–ã€ç¤¾ä¼šçƒ­ç‚¹ç»“åˆã€çŸ­è§†é¢‘ä¼ æ’­ã€‚",
        "instruction": "æ€è€ƒå¦‚ä½•å°†å­¦æœ¯å†…å®¹è½¬åŒ–ä¸ºå¤§ä¼—å¯æ„ŸçŸ¥çš„'æ•…äº‹'ã€‚å…³æ³¨æŠ–éŸ³ç­‰å¹³å°çš„ä¼ æ’­è§„å¾‹ï¼ˆå®Œæ’­ç‡ã€è¯é¢˜æ€§ï¼‰ã€‚"
    },
    "æƒ…æ„ŸåŒ–è®¾è®¡ç ”ç©¶å‘˜": {
        "description": "å¿ƒç†å­¦ä¸è®¾è®¡çš„ç»“åˆè€…ã€‚å…³æ³¨'åŠŸèƒ½'å¦‚ä½•è½¬åŒ–ä¸º'æƒ…æ„Ÿè¿æ¥'ã€‚",
        "focus": "æƒ…æ„Ÿå…±é¸£ã€å¿ƒç†å­¦ã€è¯—æ„è¡¨è¾¾ã€ç”Ÿå‘½ä½“éªŒã€‚",
        "instruction": "æŒ–æ˜æ–‡ç‰©èƒŒåçš„æƒ…æ„Ÿä»·å€¼ã€‚æ€è€ƒå¦‚ä½•é€šè¿‡è®¾è®¡å¼•å‘è§‚ä¼—çš„'æƒŠå¹'ã€'æ„ŸåŠ¨'æˆ–'æ²‰æ€'ã€‚"
    }
}

PROFESSIONAL_ANGLES = """
åœ¨åˆ†ææ—¶ï¼Œè¯·ç»¼åˆè€ƒè™‘ä»¥ä¸‹ç»´åº¦ï¼š
1. [å†å²ä¸è€ƒå¤]: å‡ºåœŸèƒŒæ™¯ã€æ–­ä»£ä¾æ®ã€ç¤¾ä¼šåˆ¶åº¦ã€‚
2. [å·¥è‰ºä¸ææ–™]: åˆ¶ä½œæ–¹æ³•ã€ææ–™æ¥æºã€æŠ€æœ¯æ°´å¹³ã€‚
3. [è‰²å½©]: é¢œè‰²ç¾å­¦ã€ç­‰çº§è±¡å¾ã€çŸ¿ç‰©é¢œæ–™ã€‚
4. [å™¨å½¢]: åŠŸèƒ½ä¸å®¡ç¾çš„ç»“åˆã€ç¤¼åˆ¶è§„èŒƒã€‚
5. [ç¬¦å·ä¸éšå–»]: çº¹æ ·çš„æ–‡åŒ–å¯“æ„ã€å®—æ•™å†…æ¶µã€‚
6. [è¯—æ„è¡¨è¾¾]: ç›¸å…³çš„è¯—è¯æ­Œèµ‹ã€æ–‡å­¦æ„è±¡ã€‚
7. [æ•°å­—ç©ºé—´]: AR/VRã€åäººæœºäº¤äº’çš„å¯èƒ½æ€§ã€‚
"""


# --- 2. æ ¸å¿ƒåŠŸèƒ½å‡½æ•° ---

def parse_and_render_message(text):
    """è§£æ <think> æ ‡ç­¾å¹¶æ¸²æŸ“"""
    pattern = r"<think>(.*?)</think>(.*)"
    match = re.search(pattern, text, re.DOTALL)
    if match:
        thought, answer = match.group(1).strip(), match.group(2).strip()
        if thought:
            with st.expander("ğŸ’­ æ¨¡å‹æ€è€ƒè¿‡ç¨‹ (ç‚¹å‡»å±•å¼€)", expanded=False):
                st.markdown(thought)
        if answer:
            st.markdown(answer)
        else:
            st.info("æ¨¡å‹ä»…è¾“å‡ºäº†æ€è€ƒè¿‡ç¨‹ã€‚")
    else:
        clean_text = text.replace("<think>", "**[å¼€å§‹æ€è€ƒ]**\n").replace("</think>", "\n**[æ€è€ƒç»“æŸ]**\n")
        st.markdown(clean_text)


# [æ–°å¢] æ ¼å¼åŒ–å†å²è®°å½•å‡½æ•°
def format_chat_history(messages, k=6):
    """å°†æœ€è¿‘çš„ k æ¡å¯¹è¯è®°å½•è½¬æ¢ä¸ºå­—ç¬¦ä¸²ï¼Œä¾›æ¨¡å‹ç†è§£ä¸Šä¸‹æ–‡"""
    recent_msgs = messages[-k:]  # åªå–æœ€è¿‘kæ¡ï¼Œé¿å…ä¸Šä¸‹æ–‡è¿‡é•¿
    history_text = ""
    for msg in recent_msgs:
        role = "ç”¨æˆ·" if msg["role"] == "user" else "AIåŠ©æ‰‹"
        content = msg["content"].replace("<think>", "").replace("</think>", "")  # æ¸…ç†thinkæ ‡ç­¾ï¼Œå‡å°‘å¹²æ‰°
        history_text += f"{role}: {content}\n"
    return history_text


# --- çŠ¶æ€ç®¡ç†å‡½æ•° ---
def load_db_status():
    """è¯»å–å·²å­¦ä¹ æ–‡ä»¶åˆ—è¡¨"""
    if os.path.exists(STATUS_FILE):
        try:
            with open(STATUS_FILE, "r", encoding="utf-8") as f:
                return set(json.load(f))
        except:
            return set()
    return set()


def update_db_status(filename, action="add"):
    """æ›´æ–°çŠ¶æ€æ–‡ä»¶"""
    current_status = load_db_status()
    if action == "add":
        current_status.add(filename)
    elif action == "remove":
        if filename in current_status:
            current_status.remove(filename)

    with open(STATUS_FILE, "w", encoding="utf-8") as f:
        json.dump(list(current_status), f, ensure_ascii=False, indent=4)


def get_vector_store():
    """è·å–å‘é‡æ•°æ®åº“å®ä¾‹"""
    embeddings = OllamaEmbeddings(base_url=OLLAMA_URL, model="nomic-embed-text")
    return Chroma(persist_directory=CHROMA_DB_DIR, embedding_function=embeddings)


def ingest_file(filename):
    """å°†æŒ‡å®šæ–‡ä»¶(å·²åœ¨æ–‡ä»¶å¤¹ä¸­)å­˜å…¥å‘é‡åº“"""
    file_path = os.path.join(KNOWLEDGE_BASE_DIR, filename)
    if not os.path.exists(file_path):
        return False, "æ–‡ä»¶ä¸å­˜åœ¨"

    try:
        # 1. åŠ è½½æ–‡ä»¶
        suffix = os.path.splitext(filename)[1].lower()
        if suffix == ".pdf":
            loader = PyPDFLoader(file_path)
        elif suffix == ".md":
            loader = UnstructuredMarkdownLoader(file_path)
        else:
            loader = TextLoader(file_path, autodetect_encoding=True)

        docs = loader.load()

        # 2. åˆ‡åˆ†æ–‡æ¡£
        text_splitter = RecursiveCharacterTextSplitter(chunk_size=800, chunk_overlap=100)
        chunks = text_splitter.split_documents(docs)

        if not chunks:
            return False, "âš ï¸ æ–‡æ¡£åˆ‡åˆ†åå†…å®¹ä¸ºç©ºã€‚"

        # æ·»åŠ å…ƒæ•°æ®
        abs_path = os.path.abspath(file_path)
        for chunk in chunks:
            chunk.metadata['source'] = abs_path

        ids = [str(uuid.uuid4()) for _ in chunks]

        # 3. å­˜å…¥æ•°æ®åº“
        vector_store = get_vector_store()
        vector_store.add_documents(chunks, ids=ids)
        vector_store.persist()

        update_db_status(filename, "add")

        return True, f"âœ… å·²å­¦ä¹ ï¼š{len(chunks)} ä¸ªçŸ¥è¯†å—"
    except Exception as e:
        import traceback
        traceback.print_exc()
        return False, f"å¤„ç†å¤±è´¥: {str(e)}"


def delete_document_complete(filename):
    """å½»åº•åˆ é™¤ï¼šåˆ å‘é‡ + åˆ æ–‡ä»¶ + åˆ çŠ¶æ€"""
    try:
        file_path = os.path.join(KNOWLEDGE_BASE_DIR, filename)
        abs_path = os.path.abspath(file_path)

        if filename in load_db_status():
            vector_store = get_vector_store()
            vector_store._collection.delete(where={"source": abs_path})
            vector_store.persist()
            update_db_status(filename, "remove")

        if os.path.exists(file_path):
            os.remove(file_path)
            return True, f"ğŸ—‘ï¸ å·²å½»åº•ç§»é™¤: {filename}"
        else:
            return True, f"âš ï¸ æ–‡ä»¶å·²ä»åº“ä¸­ç§»é™¤ï¼Œä½†ç£ç›˜ä¸Šæœªæ‰¾åˆ°: {filename}"

    except Exception as e:
        return False, f"åˆ é™¤å¤±è´¥: {str(e)}"


# å†å²è®°å½•ç®¡ç†å‡½æ•°
def save_chat_history(chat_id, messages):
    with open(os.path.join(HISTORY_DIR, f"{chat_id}.json"), "w", encoding="utf-8") as f:
        json.dump(messages, f, ensure_ascii=False, indent=4)


def load_chat_history(chat_id):
    path = os.path.join(HISTORY_DIR, f"{chat_id}.json")
    return json.load(open(path, "r", encoding="utf-8")) if os.path.exists(path) else []


def get_chat_history_list():
    files = glob.glob(os.path.join(HISTORY_DIR, "*.json"))
    files.sort(key=os.path.getmtime, reverse=True)
    return files


def get_chat_title(messages):
    for msg in messages:
        if msg["role"] == "user":
            return msg["content"][:12] + "..." if len(msg["content"]) > 12 else msg["content"]
    return "æ–°å¯¹è¯"


def init_new_chat():
    st.session_state.current_chat_id = datetime.now().strftime("%Y%m%d_%H%M%S")
    st.session_state.messages = [{"role": "assistant", "content": "ä½ å¥½ï¼æˆ‘æ˜¯å›½å®æ´»åŒ–åŠ©æ‰‹ã€‚è¯·ä¸Šä¼ èµ„æ–™æˆ–ç›´æ¥æé—®ã€‚"}]


# --- 3. åˆå§‹åŒ– Session ---
if "current_chat_id" not in st.session_state:
    init_new_chat()


# --- 4. åŠ è½½èµ„æº ---
@st.cache_resource
def load_resources():
    embeddings = OllamaEmbeddings(base_url=OLLAMA_URL, model="nomic-embed-text")
    llm = ChatOllama(base_url=OLLAMA_URL, model="qwen3:14b", temperature=0.5)
    return embeddings, llm


try:
    embeddings, llm = load_resources()
except Exception as e:
    st.error(f"æ¨¡å‹è¿æ¥å¤±è´¥: {e}")
    st.stop()

# --- 5. ä¾§è¾¹æ  UI ---
with st.sidebar:
    st.title("ğŸº å›½å®AIæ´»åŒ–")

    # === æ¨¡å— A: çŸ¥è¯†åº“å…¨æµç¨‹ç®¡ç† ===
    with st.expander("ğŸ“š èµ„æ–™åº“ç®¡ç†", expanded=True):
        uploaded_file = st.file_uploader("ä¸Šä¼ æ–°èµ„æ–™", type=["pdf", "md", "txt"])
        if uploaded_file:
            save_path = os.path.join(KNOWLEDGE_BASE_DIR, uploaded_file.name)
            with open(save_path, "wb") as f:
                f.write(uploaded_file.getbuffer())
            st.toast(f"æ–‡ä»¶ {uploaded_file.name} å·²ä¿å­˜ï¼Œè¯·ç‚¹å‡»'å­¦ä¹ 'å…¥åº“ã€‚", icon="ğŸ’¾")
            time.sleep(1)
            st.rerun()

        st.divider()
        st.caption("ğŸ“‚ èµ„æ–™åº“åˆ—è¡¨")

        if os.path.exists(KNOWLEDGE_BASE_DIR):
            all_files = sorted(os.listdir(KNOWLEDGE_BASE_DIR))
        else:
            all_files = []

        learned_status = load_db_status()

        if not all_files:
            st.info("æš‚æ— æ–‡æ¡£")
        else:
            for filename in all_files:
                col_icon, col_name, col_btn = st.columns([0.15, 0.65, 0.2])
                is_learned = filename in learned_status

                with col_icon:
                    st.write("âœ…" if is_learned else "âšª")

                with col_name:
                    st.text(filename)

                with col_btn:
                    if not is_learned:
                        if st.button("å­¦ä¹ ", key=f"learn_{filename}", help="ç‚¹å‡»å…¥åº“"):
                            with st.spinner("æ­£åœ¨å­¦ä¹ ..."):
                                success, msg = ingest_file(filename)
                                if success:
                                    st.toast(msg, icon="ğŸ‰")
                                    st.rerun()
                                else:
                                    st.error(msg)
                    else:
                        if st.button("ğŸ—‘ï¸", key=f"del_{filename}", help="å½»åº•åˆ é™¤"):
                            with st.spinner("æ­£åœ¨æ¸…ç†..."):
                                success, msg = delete_document_complete(filename)
                                if success:
                                    st.toast(msg, icon="ğŸ‘‹")
                                    st.rerun()
                                else:
                                    st.error(msg)

            unlearned_count = len([f for f in all_files if f not in learned_status])
            if unlearned_count > 0:
                st.divider()
                if st.button(f"ğŸš€ ä¸€é”®å­¦ä¹ å‰©ä½™ {unlearned_count} ä¸ªæ–‡ä»¶", type="primary"):
                    progress_bar = st.progress(0)
                    for i, fname in enumerate(all_files):
                        if fname not in learned_status:
                            ingest_file(fname)
                        progress_bar.progress((i + 1) / len(all_files))
                    st.success("å…¨éƒ¨å…¥åº“å®Œæˆï¼")
                    time.sleep(1)
                    st.rerun()

    st.divider()

    # === æ¨¡å— B: å¯¹è¯ç®¡ç† ===
    if st.button("â• æ–°å»ºå¯¹è¯", use_container_width=True):
        init_new_chat()
        st.rerun()

    st.subheader("ğŸ“œ å†å²è®°å½•")
    files = get_chat_history_list()
    if not files: st.caption("æš‚æ— è®°å½•")

    for file_path in files:
        file_name = os.path.basename(file_path)
        chat_id = file_name.replace(".json", "")
        try:
            with open(file_path, "r", encoding="utf-8") as f:
                msgs = json.load(f)
            title = get_chat_title(msgs)
            date_str = f"{chat_id[4:6]}/{chat_id[6:8]} {chat_id[9:11]}:{chat_id[11:13]}"
        except:
            title = "æœªçŸ¥å¯¹è¯"
            date_str = ""

        col1, col2 = st.columns([0.85, 0.15])
        with col1:
            prefix = "ğŸ“‚ " if st.session_state.current_chat_id == chat_id else ""
            if st.button(f"{prefix}{title}\n{date_str}", key=f"load_{chat_id}", use_container_width=True):
                st.session_state.current_chat_id = chat_id
                st.session_state.messages = msgs
                st.rerun()
        with col2:
            if st.button("ğŸ—‘", key=f"del_{chat_id}"):
                os.remove(file_path)
                if st.session_state.current_chat_id == chat_id: init_new_chat()
                st.rerun()

    st.divider()

    # === æ¨¡å— C: è§’è‰²é€‰æ‹© ===
    selected_role = st.selectbox("ğŸ­ é€‰æ‹©åˆ†æè§’è‰²", list(ROLE_DEFINITIONS.keys()))
    role_info = ROLE_DEFINITIONS[selected_role]
    st.info(f"**{selected_role}**\n\n{role_info['description']}")

# --- 6. ä¸»ç•Œé¢ä¸ RAG é€»è¾‘ ---

st.header(f"å½“å‰ä¼šè¯: {get_chat_title(st.session_state.messages)}")

if os.path.exists(CHROMA_DB_DIR):
    vector_store = get_vector_store()
    try:
        cnt = vector_store._collection.count()
        k = min(4, cnt) if cnt > 0 else 0
    except:
        k = 0

    if k > 0:
        retriever = vector_store.as_retriever(search_kwargs={"k": k})
    else:
        retriever = None
else:
    retriever = None

# æ¸²æŸ“å†å²æ¶ˆæ¯
for msg in st.session_state.messages:
    with st.chat_message(msg["role"]):
        if msg["role"] == "assistant":
            parse_and_render_message(msg["content"])
        else:
            st.markdown(msg["content"])

role_config = ROLE_DEFINITIONS[selected_role]

# [ä¿®æ”¹] å®šä¹‰æŸ¥è¯¢é‡å†™ (Contextualize) çš„ Prompt
# ç›®çš„ï¼šå°†ç”¨æˆ·çš„ "å®ƒ"ã€"é‚£ä¸ª" ç­‰ä»£è¯æ›¿æ¢ä¸ºå†å²ä¸­çš„å…·ä½“åè¯
rephrase_prompt_template = """
ç»™å®šä»¥ä¸‹å¯¹è¯å†å²å’Œç”¨æˆ·çš„æœ€æ–°é—®é¢˜ï¼Œè¯·å°†ç”¨æˆ·çš„æœ€æ–°é—®é¢˜æ”¹å†™ä¸ºä¸€ä¸ª**ç‹¬ç«‹ã€å®Œæ•´ã€ä¸ä¾èµ–ä¸Šä¸‹æ–‡å³å¯ç†è§£çš„é—®é¢˜**ã€‚
å¦‚æœç”¨æˆ·çš„é—®é¢˜å·²ç»å¾ˆå®Œæ•´ï¼Œç›´æ¥è¿”å›åŸé—®é¢˜ã€‚
ä¸è¦å›ç­”é—®é¢˜ï¼Œåªè´Ÿè´£æ”¹å†™ã€‚ä¸è¦è¾“å‡ºä»»ä½•æ€è€ƒè¿‡ç¨‹æˆ–æ ‡ç­¾ã€‚

å¯¹è¯å†å²ï¼š
{chat_history}

ç”¨æˆ·æœ€æ–°é—®é¢˜ï¼š{question}

ç‹¬ç«‹é—®é¢˜ï¼š
"""
rephrase_prompt = PromptTemplate(
    input_variables=["chat_history", "question"],
    template=rephrase_prompt_template
)

# [ä¿®æ”¹] å®šä¹‰æœ€ç»ˆå›ç­”çš„ Prompt (åŠ å…¥ chat_history)
system_template = f"""
ä½ ç°åœ¨çš„èº«ä»½æ˜¯ï¼š**{selected_role}**ã€‚
{role_config['description']}

**ä½ çš„æ ¸å¿ƒå…³æ³¨ç‚¹ï¼š**
{role_config['focus']}

**å›å¤æŒ‡å¯¼åŸåˆ™ï¼š**
{role_config['instruction']}

**ä¸“ä¸šåˆ†æç»´åº¦å‚è€ƒï¼š**
{PROFESSIONAL_ANGLES}

è¯·æ ¹æ®ä»¥ä¸‹ã€å‚è€ƒèµ„æ–™ã€‘å’Œã€å¯¹è¯å†å²ã€‘æ¥å›ç­”ç”¨æˆ·çš„ã€æœ€æ–°é—®é¢˜ã€‘ã€‚
å¦‚æœèµ„æ–™ä¸­æ²¡æœ‰ç­”æ¡ˆï¼Œè¯·è¿ç”¨ä½ çš„ä¸“ä¸šçŸ¥è¯†è¿›è¡Œåˆç†æ¨æ¼”ï¼Œä½†å¿…é¡»å£°æ˜è¿™æ˜¯æ¨æ¼”ã€‚

---
ã€å¯¹è¯å†å²ã€‘ï¼š
{{chat_history}}

ã€å‚è€ƒèµ„æ–™ã€‘ï¼š
{{context}}
---
ã€æœ€æ–°é—®é¢˜ã€‘ï¼š
{{question}}
"""

final_prompt = ChatPromptTemplate.from_template(system_template)

if user_input := st.chat_input("è¯·è¾“å…¥å…³äºæ–‡ç‰©çš„åˆ›æ„æˆ–é—®é¢˜..."):
    with st.chat_message("user"):
        st.markdown(user_input)
    # åœ¨ç”Ÿæˆå‰å…ˆä¸åŠ å…¥ session_stateï¼Œç­‰ç”Ÿæˆå®Œå†åŠ ï¼Œæˆ–è€…ç°åœ¨åŠ ä¹Ÿå¯ä»¥ï¼Œ
    # è¿™é‡Œä¸ºäº†ä¿æŒé€»è¾‘ä¸€è‡´ï¼Œæˆ‘ä»¬æ‰‹åŠ¨ç»´æŠ¤ç»™æ¨¡å‹çš„ historyï¼Œä¸åŒ…å«å½“å‰è¿™å¥ user_input

    # 1. å‡†å¤‡å†å²è®°å½•æ–‡æœ¬
    history_text = format_chat_history(st.session_state.messages, k=6)

    with st.chat_message("assistant"):
        placeholder = st.empty()
        full_response = ""

        try:
            with st.spinner(f"{selected_role} æ­£åœ¨æ€è€ƒ..."):

                # --- [æ­¥éª¤ 1]: ä¸Šä¸‹æ–‡ç†è§£ä¸æŸ¥è¯¢é‡å†™ ---
                # å¦‚æœæœ‰å†å²è®°å½•ï¼Œå…ˆè¿›è¡Œé‡å†™ï¼›å¦‚æœæ˜¯ç¬¬ä¸€å¥è¯ï¼Œç›´æ¥ç”¨åŸè¯
                actual_query = user_input
                if len(st.session_state.messages) > 1:
                    rephrase_chain = rephrase_prompt | llm | StrOutputParser()
                    reformulated_question = rephrase_chain.invoke({
                        "chat_history": history_text,
                        "question": user_input
                    })
                    # æ¸…ç†å¯èƒ½äº§ç”Ÿçš„å¤šä½™ç©ºç™½
                    actual_query = reformulated_question.strip()

                    # è°ƒè¯•ä¿¡æ¯ï¼šå±•ç¤ºé‡å†™åçš„é—®é¢˜ï¼ˆå¯é€‰ï¼Œè§‰å¾—ä¸éœ€è¦å¯ä»¥æ³¨é‡Šæ‰ï¼‰
                    with st.expander("ğŸ” ä¸Šä¸‹æ–‡ç†è§£ (æŸ¥è¯¢é‡å†™)", expanded=False):
                        st.write(f"åŸé—®é¢˜: {user_input}")
                        st.write(f"ç†è§£ä¸º: {actual_query}")

                # --- [æ­¥éª¤ 2]: æ£€ç´¢ ---
                context_text = ""
                if retriever:
                    # ä½¿ç”¨é‡å†™åçš„é—®é¢˜å»æ£€ç´¢
                    docs = retriever.get_relevant_documents(actual_query)
                    context_text = "\n\n".join([d.page_content for d in docs])

                if not context_text:
                    context_text = "æš‚æ— æœ¬åœ°çŸ¥è¯†åº“ç›¸å…³å†…å®¹ï¼Œè¯·ä¾é ä½ çš„é€šç”¨çŸ¥è¯†å›ç­”ã€‚"

                # --- [æ­¥éª¤ 3]: ç”Ÿæˆå›ç­” ---
                # å°† é‡å†™åçš„é—®é¢˜(æˆ–è€…åŸé—®é¢˜) + æ£€ç´¢åˆ°çš„ä¸Šä¸‹æ–‡ + å†å²è®°å½• ä¼ ç»™æœ€ç»ˆ Prompt
                # æ³¨æ„ï¼šè¿™é‡Œæˆ‘ä»¬é€šå¸¸æŠŠ user_input ä¼ ç»™ Prompt æ˜¾ç¤ºç»™ç”¨æˆ·çœ‹ï¼Œ
                # ä½†å®é™…ä¸Š retrieve ç”¨çš„æ˜¯ actual_queryã€‚
                # æœ‰ä¸€ç§åšæ³•æ˜¯ Prompt é‡Œä¹Ÿæ”¾ actual_queryï¼Œä½†ä¸ºäº†ä¿æŒå¯¹è¯è‡ªç„¶ï¼Œ
                # æˆ‘ä»¬Prompté‡Œè¿˜æ˜¯æ”¾ user_inputï¼Œå› ä¸ºä¸Šä¸‹æ–‡éƒ½åœ¨ chat_history é‡Œäº†ï¼Œ
                # ä¸»è¦æ˜¯ä¸ºäº†è®© Context (å‚è€ƒèµ„æ–™) æ˜¯å‡†ç¡®çš„ã€‚

                chain = (
                        final_prompt
                        | llm
                        | StrOutputParser()
                )

                # æµå¼è¾“å‡º
                stream_input = {
                    "chat_history": history_text,
                    "context": context_text,
                    "question": user_input  # è¿™é‡Œç”¨åŸè¯ï¼Œå› ä¸ºPrompté‡Œæœ‰Historyå…œåº•ï¼Œä¸”Retrieverå·²ç»ç”¨actual_queryæ‰¾è¿‡èµ„æ–™äº†
                }

                for chunk in chain.stream(stream_input):
                    full_response += chunk
                    placeholder.markdown(full_response + "â–Œ")

                placeholder.empty()
                parse_and_render_message(full_response)

        except Exception as e:
            import traceback

            traceback.print_exc()
            st.error(f"ç”Ÿæˆå‡ºé”™: {e}")
            full_response = "æŠ±æ­‰ï¼Œç³»ç»Ÿå‡ºäº†ç‚¹å°å·®é”™ã€‚"

    # æ›´æ–° Session State
    st.session_state.messages.append({"role": "user", "content": user_input})
    st.session_state.messages.append({"role": "assistant", "content": full_response})
    save_chat_history(st.session_state.current_chat_id, st.session_state.messages)